ğŸ–ï¸ PROVE: 3D Hand Angles Plotting â€” Right Index Finger Kinematics
This project computes, analyzes, and visualizes the 3D joint angles of the right index finger derived from motion-capture typing data. By extracting MCP abduction, MCP/PIP/DIP flexion, and angular velocity, PROVE (Project for Verification of Kinematic Events) analyzes biomechanical typing patterns to reveal motor intent and keypress identity.

ğŸ§  Motivation: Decoding Biomechanical Signatures
Typing generates distinct biomechanical signatures in finger movement. This project investigates whether fine-grained finger-joint kinematics can reliably predict which key is being pressed, opening doors for advanced Human-Computer Interaction (HCI) applications:

ğŸ‘ï¸+âœ‹ Gaze-and-Hand Predictive Typing: Enhancing predictive text models by incorporating sub-conscious finger movements.

ğŸ§  Motor-Intention Decoding: Translating pre-press movement into input for control or accessibility.

ğŸ•¶ï¸ VR/AR Natural Text Entry: Developing robust, intuitive input systems for immersive environments.

ğŸ”¬ Biomechanics & HCI Research: Advancing the fundamental understanding of human motor control during interaction.

âœ¨ Features
Kinematic Extraction: Computation of right-index-finger MCP abduction and MCP/PIP/DIP flexion.

Dynamic Analysis: Calculation of angular velocity (Ï‰) using angle unwrapping for continuous data.

Event Sampling: Precise sampling of angles and velocities at per-keypress events.

3D Visualization: Plotting of kinematic clusters for y,h,n,u,j,m keys to demonstrate key separation.

High-Throughput Processing: Parallel CSV processing and support for the QTM motion-capture marker format.

ğŸ“ Mathematical Methodology
The project relies on standard vector geometry to calculate joint angles relative to the palm plane.

Key Formulas

Kinematic Measure	Description	Formula
Palm Plane Normal ( 
n
^
 )	Unit vector perpendicular to the palm's surface.	
n
^
 = 
âˆ¥â€¦âˆ¥
(p 
2
â€‹	
 âˆ’p 
1
â€‹	
 )Ã—(p 
3
â€‹	
 âˆ’p 
1
â€‹	
 )
â€‹	
 
MCP Abduction Angle	Signed angle between the proximal phalanx projection and a reference vector on the palm plane.	
Î¸ 
abd
â€‹	
 =sign(( 
v

  
ref
â€‹	
 Ã— 
v

  
proj
â€‹	
 )â‹… 
n
^
 )â‹…Î¸
PIP / DIP Flexion	Angle between adjacent bone segment vectors ( 
v

  
1
â€‹	
 , 
v

  
2
â€‹	
 ).	
Î¸ 
flex
â€‹	
 =arccos( 
âˆ¥ 
v

  
1
â€‹	
 âˆ¥âˆ¥ 
v

  
2
â€‹	
 âˆ¥
v

  
1
â€‹	
 â‹… 
v

  
2
â€‹	
 
â€‹	
 )
Angular Velocity (Ï‰)	Rate of change of the unwrapped joint angle.	
Ï‰= 
dt
d(Î¸ 
unwrap
â€‹	
 )
â€‹	
 
ğŸ“Š Visualizations
The output focuses on 3D clustering and dynamic phase space analysis to reveal distinct typing patterns.

Plot Type	Axes/Dimensions	Purpose
3D MCP Space	Abduction Ã— Flexion Ã— Velocity	Visualize the complete dynamic motion space of the MCP joint for each key.
3D Flexion Clusters	MCP vs PIP vs DIP	Analyze the coordination and inter-dependency between the three main finger joints.
Flexionâ€“Velocity Grid	Dynamic Phase Plots	Show the rate of change of flexion against the angle itself.
ğŸ“‚ Input Data Format
The system expects motion-capture data structured in CSV files, including:

XYZ Coordinates for all index finger and palm markers.

TimeStamp (Frame time).

Pressed_Letter (The character associated with the event).

KeyPressFlag (A binary marker for keystroke contact).

âš™ï¸ Processing Pipeline
Data Loading: Ingest QTM motion-capture data.

Vector Calculation: Compute segment vectors and the Palm Plane Normal ( 
n
^
 ).

Angle Calculation: Determine raw MCP, PIP, and DIP joint angles.

Signal Processing: Apply angle unwrapping and conversion to degrees.

Velocity Calculation: Compute angular velocity (Ï‰).

Event Sampling: Sample angle/velocity data precisely at keystrokes.

Visualization: Generate 3D kinematic cluster plots.

â–¶ï¸ Run Instructions
To run the analysis script:

Bash
# Ensure dependencies are installed
pip install numpy pandas matplotlib seaborn concurrent.futures

# Execute the main script
python PROVE.py
Dependencies

numpy

pandas

matplotlib

seaborn

concurrent.futures

ğŸ“ Author
Fateme Eslami AI & Motion Interaction Research University of Birmingham
